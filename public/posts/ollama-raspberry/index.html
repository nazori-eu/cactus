<!DOCTYPE html>
<html lang="en-us">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-regular-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/font-awesome/webfonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="preload" href="/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  
  <script type="text/javascript" src="https://latest.cactus.chat/cactus.js"></script>
  <link rel="stylesheet" href="https://latest.cactus.chat/style.css" type="text/css">
  
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> Ollama in Rasberry Pi 5 | nazori Archive</title>
  <link rel = 'canonical' href = 'http://localhost:1313/posts/ollama-raspberry/'>
  <meta name="description" content="This web is just for my personal long term memory.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta property="og:url" content="http://localhost:1313/posts/ollama-raspberry/">
  <meta property="og:site_name" content="nazori Archive">
  <meta property="og:title" content="Ollama in Rasberry Pi 5">
  <meta property="og:description" content="What is Ollama? Ollama is a platform designed to run state-of-the-art AI models locally on your device. It simplifies the process of deploying large language models (LLMs) like Qwen-2.5 (3B) and Gemma2 (2B) on systems like the Raspberry Pi 5. By supporting both low- and high-complexity models, Ollama lets users run sophisticated AI directly on their devices without needing cloud services.
With Ollama running on your Raspberry Pi 5, you can have easy access to AI for tasks like generating text, answering questions, and more—all with a local, efficient setup.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-01-21T09:00:00+00:00">
    <meta property="article:modified_time" content="2025-01-21T09:00:00+00:00">
    <meta property="article:tag" content="Homelab">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Ollama in Rasberry Pi 5">
  <meta name="twitter:description" content="What is Ollama? Ollama is a platform designed to run state-of-the-art AI models locally on your device. It simplifies the process of deploying large language models (LLMs) like Qwen-2.5 (3B) and Gemma2 (2B) on systems like the Raspberry Pi 5. By supporting both low- and high-complexity models, Ollama lets users run sophisticated AI directly on their devices without needing cloud services.
With Ollama running on your Raspberry Pi 5, you can have easy access to AI for tasks like generating text, answering questions, and more—all with a local, efficient setup.">

  
  
    
  
  
  <link rel="stylesheet" href="http://localhost:1313/css/styles.54711f9276af3be58509ef2632336324a01036f21ac4ae082c1e44cc4f1329c8e4a4aaf77092605a5389f53a6eea78494e54831503c7e37897a83d1ef3c1b01a.css" integrity="sha512-VHEfknavO&#43;WFCe8mMjNjJKAQNvIaxK4ILB5EzE8TKcjkpKr3cJJgWlOJ9Tpu6nhJTlSDFQPH43iXqD0e88GwGg=="> 

  
  
  
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
<link rel="icon" type="image/png" href="http://localhost:1313/images/favicon.ico" />

  
  
  
</head>

<body class="max-width mx-auto px3 ltr">
  <div class="content index py4">

  <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;" aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/posts">Writings</a></li>
         
        <li><a href="/readings">Readings</a></li>
         
        <li><a href="/categories">Categories</a></li>
         
        <li><a href="/about">About</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li>
          <a class="icon" href=" http://localhost:1313/posts/ssh-tunneling/" aria-label="Previous">
            <i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i>
          </a>
        </li>
        
        
        <li>
          <a class="icon" href="http://localhost:1313/readings/" aria-label="Next">
            <i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i>
          </a>
        </li>
        
        <li>
          <a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
            <i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i>
          </a>
        </li>
        <li>
          <a class="icon" href="#" aria-label="Share">
            <i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i>
          </a>
        </li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f" aria-label="Facebook">
      <i class="fab fa-facebook " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&text=Ollama%20in%20Rasberry%20Pi%205" aria-label="Twitter">
      <i class="fab fa-twitter " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&title=Ollama%20in%20Rasberry%20Pi%205" aria-label="Linkedin">
      <i class="fab fa-linkedin " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&is_video=false&description=Ollama%20in%20Rasberry%20Pi%205" aria-label="Pinterest">
      <i class="fab fa-pinterest " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=Ollama%20in%20Rasberry%20Pi%205&body=Check out this article: http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f" aria-label="Email">
      <i class="fas fa-envelope " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&title=Ollama%20in%20Rasberry%20Pi%205" aria-label="Pocket">
      <i class="fab fa-get-pocket " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&title=Ollama%20in%20Rasberry%20Pi%205" aria-label="reddit">
      <i class="fab fa-reddit " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&name=Ollama%20in%20Rasberry%20Pi%205&description=%3ch2%20id%3d%22what-is-ollama%22%3eWhat%20is%20Ollama%3f%3c%2fh2%3e%0a%3cp%3eOllama%20is%20a%20platform%20designed%20to%20run%20state-of-the-art%20AI%20models%20locally%20on%20your%20device.%20It%20simplifies%20the%20process%20of%20deploying%20large%20language%20models%20%28LLMs%29%20like%20%3cstrong%3eQwen-2.5%20%283B%29%3c%2fstrong%3e%20and%20%3cstrong%3eGemma2%20%282B%29%3c%2fstrong%3e%20on%20systems%20like%20the%20%3cstrong%3eRaspberry%20Pi%205%3c%2fstrong%3e.%20By%20supporting%20both%20low-%20and%20high-complexity%20models%2c%20Ollama%20lets%20users%20run%20sophisticated%20AI%20directly%20on%20their%20devices%20without%20needing%20cloud%20services.%3c%2fp%3e%0a%3cp%3eWith%20%3cstrong%3eOllama%3c%2fstrong%3e%20running%20on%20your%20Raspberry%20Pi%205%2c%20you%20can%20have%20easy%20access%20to%20AI%20for%20tasks%20like%20generating%20text%2c%20answering%20questions%2c%20and%20more%e2%80%94all%20with%20a%20local%2c%20efficient%20setup.%3c%2fp%3e" aria-label="Tumblr">
      <i class="fab fa-tumblr " aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&t=Ollama%20in%20Rasberry%20Pi%205" aria-label="Hacker News">
      <i class="fab fa-hacker-news " aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>
    
    <div id="toc">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-ollama">What is Ollama?</a>
      <ul>
        <li><a href="#installing-ollama-on-raspberry-pi-5">Installing Ollama on Raspberry Pi 5</a></li>
        <li><a href="#running-models-with-ollama">Running Models with Ollama</a></li>
      </ul>
    </li>
    <li><a href="#qwen-25-3b-on-raspberry-pi-5">Qwen-2.5 (3B) on Raspberry Pi 5</a></li>
    <li><a href="#gemma2-2b-on-raspberry-pi-5">Gemma2 (2B) on Raspberry Pi 5</a></li>
  </ul>
</nav>
    </div>
    
  </span>
</div>


  <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
    <header>
      <h1 class="posttitle" itemprop="name headline">
        Ollama in Rasberry Pi 5
      </h1>
      <div class="meta">
        
        <div class="postdate">
          
          <time datetime="2025-01-21 09:00:00 &#43;0000 UTC" itemprop="datePublished">2025-01-21</time>
          
        </div>
        
        
        <div class="article-read-time">
          <i class="far fa-clock"></i>
          
          3 minute read
        </div>
        
        
        <div class="article-category">
            <i class="fas fa-archive"></i>
            
            
            <a class="category-link" href="/categories/homelab">homelab</a>
            
        </div>
        
        
        <div class="article-tag">
            <i class="fas fa-tag"></i>
            
            
            <a class="tag-link" href="/tags/homelab" rel="tag">homelab</a>
            
        </div>
        
      </div>
    </header>

  
    
    <div class="content" itemprop="articleBody">
      <h2 id="what-is-ollama">What is Ollama?</h2>
<p>Ollama is a platform designed to run state-of-the-art AI models locally on your device. It simplifies the process of deploying large language models (LLMs) like <strong>Qwen-2.5 (3B)</strong> and <strong>Gemma2 (2B)</strong> on systems like the <strong>Raspberry Pi 5</strong>. By supporting both low- and high-complexity models, Ollama lets users run sophisticated AI directly on their devices without needing cloud services.</p>
<p>With <strong>Ollama</strong> running on your Raspberry Pi 5, you can have easy access to AI for tasks like generating text, answering questions, and more—all with a local, efficient setup.</p>
<h3 id="installing-ollama-on-raspberry-pi-5">Installing Ollama on Raspberry Pi 5</h3>
<p>To run Ollama on your Raspberry Pi 5, follow these steps:</p>
<ol>
<li>
<p><strong>Update and upgrade your Raspberry Pi</strong>:
Open a terminal and run the following commands to make sure your Raspberry Pi 5 is up to date:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>sudo apt update
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span>sudo apt upgrade -y
</span></span></code></pre></div></li>
<li>
<p><strong>Install dependencies</strong>:
Ollama requires some dependencies to run, including <strong>curl</strong> and <strong>systemd</strong>. Install them by running:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>sudo apt install curl systemd -y
</span></span></code></pre></div></li>
<li>
<p><strong>Download the Ollama installation script</strong>:
Ollama provides an installation script that will set up everything you need. Download and run it with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>curl -sSL https://ollama.com/install.sh | sudo bash
</span></span></code></pre></div></li>
<li>
<p><strong>Verify the installation</strong>:
Once the script finishes, verify that Ollama was successfully installed by running:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>ollama --version
</span></span></code></pre></div><p>This should show the version of Ollama that is installed.</p>
</li>
<li>
<p><strong>Start Ollama</strong>:
After installation, Ollama should start automatically as a system service. You can confirm it’s running by checking the status:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>sudo systemctl status ollama
</span></span></code></pre></div><p>If it&rsquo;s not running, start it manually with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>sudo systemctl start ollama
</span></span></code></pre></div></li>
</ol>
<h3 id="running-models-with-ollama">Running Models with Ollama</h3>
<p>Once Ollama is installed and running, you can start pulling models and using them on your Raspberry Pi 5.</p>
<ol>
<li>
<p><strong>Pull a model</strong>:
You can use Ollama to pull models like <strong>Qwen-2.5 (3B)</strong> or <strong>Gemma2 (2B)</strong>. For example, to pull the <strong>Qwen-2.5 (3B)</strong> model, run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>ollama pull qwen-2.5:3b
</span></span></code></pre></div></li>
<li>
<p><strong>Run a model</strong>:
Once the model is pulled, you can run it interactively. For instance, to run the <strong>Qwen-2.5 (3B)</strong> model, use the following command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>ollama run qwen-2.5:3b
</span></span></code></pre></div><p>This will open an interactive session where you can type in queries or prompts, and the model will generate responses.</p>
</li>
<li>
<p><strong>Running Gemma2 (2B)</strong>:
Similarly, to pull and run <strong>Gemma2 (2B)</strong>, use:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>ollama pull gemma2:2b
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span>ollama run gemma2:2b
</span></span></code></pre></div></li>
</ol>
<h2 id="qwen-25-3b-on-raspberry-pi-5">Qwen-2.5 (3B) on Raspberry Pi 5</h2>
<p><strong>Qwen-2.5 (3B)</strong> is a robust language model designed to handle a wide range of natural language processing (NLP) tasks. It is the third iteration of the <strong>Qwen series</strong>, developed by <strong>Qianwen AI</strong>. This model is designed for high-performance environments and is capable of complex text generation, conversation, and contextual understanding. The <strong>3B</strong> in the name refers to the <strong>3 billion parameters</strong> the model uses, which allows it to have a better grasp of nuanced language, making it suitable for advanced applications in text generation and language comprehension.</p>
<p>The model was specifically created to balance computational efficiency with high-quality output, meaning it can deliver impressive performance without requiring the immense hardware demands of even larger models. For Raspberry Pi 5 users, it provides a powerful option to run sophisticated AI models locally, without relying on cloud-based solutions.</p>
<p>Despite being a high-performance model, it is still optimized enough to work on edge devices like the <strong>Raspberry Pi 5</strong>, which boasts improved CPU and GPU performance over previous versions. Running <strong>Qwen-2.5 (3B)</strong> on the Raspberry Pi 5 opens up opportunities for building advanced conversational agents, content generation tools, and more.</p>
<h2 id="gemma2-2b-on-raspberry-pi-5">Gemma2 (2B) on Raspberry Pi 5</h2>
<p><strong>Gemma2 (2B)</strong> is a smaller, more lightweight model compared to <strong>Qwen-2.5 (3B)</strong>, and it&rsquo;s designed specifically to perform well in resource-constrained environments. The <strong>2B</strong> refers to its <strong>2 billion parameters</strong>, making it less resource-intensive than Qwen-2.5 but still highly capable in many NLP tasks.</p>
<p>Developed by <strong>Gemma AI</strong>, the <strong>Gemma2 (2B)</strong> model is a second-generation language model built for smaller-scale applications like conversational AI and automated text generation. Despite its smaller size, the model is still quite powerful, offering impressive text generation and understanding capabilities for its parameter count. It was optimized to run efficiently on devices like the <strong>Raspberry Pi 5</strong>, making it ideal for users who need a model that doesn&rsquo;t require the immense hardware resources of larger models but still delivers strong performance.</p>
<p>Because of its efficiency, <strong>Gemma2 (2B)</strong> is perfect for running in environments like home automation systems, chatbot applications, or any context where a lighter model with fast response times is needed. Its smaller footprint allows it to run smoothly on the Raspberry Pi 5, giving users access to powerful AI without the need for external cloud-based solutions.</p>

    </div>
  </article>

  
  




  <div class="blog-post-comments">
    
      <div id="cactus-comments-thread">
  <script>
  initComments({
    node: document.getElementById("cactus-comments-thread"),
    defaultHomeserverUrl: 'https:\/\/matrix.cactus.chat:8448',
    serverName: 'cactus.chat',
    siteName: "your_cactus_comments_sitename",
    commentSectionId: "\/posts\/ollama-raspberry\/"
  })
</script>
</div>

    
  </div>



  <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/posts">Writings</a></li>
         
          <li><a href="/readings">Readings</a></li>
         
          <li><a href="/categories">Categories</a></li>
         
          <li><a href="/about">About</a></li>
        
      </ul>
    </div>

    
    <div id="toc-footer" style="display: none">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-ollama">What is Ollama?</a>
      <ul>
        <li><a href="#installing-ollama-on-raspberry-pi-5">Installing Ollama on Raspberry Pi 5</a></li>
        <li><a href="#running-models-with-ollama">Running Models with Ollama</a></li>
      </ul>
    </li>
    <li><a href="#qwen-25-3b-on-raspberry-pi-5">Qwen-2.5 (3B) on Raspberry Pi 5</a></li>
    <li><a href="#gemma2-2b-on-raspberry-pi-5">Gemma2 (2B) on Raspberry Pi 5</a></li>
  </ul>
</nav>
    </div>
    

    <div id="share-footer" style="display: none">
      
      <ul>
  
  
    
  
  
  <li>
    <a class="icon" href="http://www.facebook.com/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f" aria-label="Facebook">
      <i class="fab fa-facebook fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://twitter.com/share?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&text=Ollama%20in%20Rasberry%20Pi%205" aria-label="Twitter">
      <i class="fab fa-twitter fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.linkedin.com/shareArticle?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&title=Ollama%20in%20Rasberry%20Pi%205" aria-label="Linkedin">
      <i class="fab fa-linkedin fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&is_video=false&description=Ollama%20in%20Rasberry%20Pi%205" aria-label="Pinterest">
      <i class="fab fa-pinterest fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="mailto:?subject=Ollama%20in%20Rasberry%20Pi%205&body=Check out this article: http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f" aria-label="Email">
      <i class="fas fa-envelope fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://getpocket.com/save?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&title=Ollama%20in%20Rasberry%20Pi%205" aria-label="Pocket">
      <i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&title=Ollama%20in%20Rasberry%20Pi%205" aria-label="reddit">
      <i class="fab fa-reddit fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="http://www.tumblr.com/share/link?url=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&name=Ollama%20in%20Rasberry%20Pi%205&description=%3ch2%20id%3d%22what-is-ollama%22%3eWhat%20is%20Ollama%3f%3c%2fh2%3e%0a%3cp%3eOllama%20is%20a%20platform%20designed%20to%20run%20state-of-the-art%20AI%20models%20locally%20on%20your%20device.%20It%20simplifies%20the%20process%20of%20deploying%20large%20language%20models%20%28LLMs%29%20like%20%3cstrong%3eQwen-2.5%20%283B%29%3c%2fstrong%3e%20and%20%3cstrong%3eGemma2%20%282B%29%3c%2fstrong%3e%20on%20systems%20like%20the%20%3cstrong%3eRaspberry%20Pi%205%3c%2fstrong%3e.%20By%20supporting%20both%20low-%20and%20high-complexity%20models%2c%20Ollama%20lets%20users%20run%20sophisticated%20AI%20directly%20on%20their%20devices%20without%20needing%20cloud%20services.%3c%2fp%3e%0a%3cp%3eWith%20%3cstrong%3eOllama%3c%2fstrong%3e%20running%20on%20your%20Raspberry%20Pi%205%2c%20you%20can%20have%20easy%20access%20to%20AI%20for%20tasks%20like%20generating%20text%2c%20answering%20questions%2c%20and%20more%e2%80%94all%20with%20a%20local%2c%20efficient%20setup.%3c%2fp%3e" aria-label="Tumblr">
      <i class="fab fa-tumblr fa-lg" aria-hidden="true"></i>
    </a>
  </li>
  <li>
    <a class="icon" href="https://news.ycombinator.com/submitlink?u=http%3a%2f%2flocalhost%3a1313%2fposts%2follama-raspberry%2f&t=Ollama%20in%20Rasberry%20Pi%205" aria-label="Hacker News">
      <i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i>
    </a>
  </li>
</ul>

    </div>

    <div id="actions-footer">
      
        <a id="menu-toggle" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;" aria-label="Menu">
          <i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
        <a id="toc-toggle" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;" aria-label="TOC">
          <i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share-toggle" class="icon" href="#" onclick="$('#share-footer').toggle();return false;" aria-label="Share">
          <i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" aria-label="Top of Page">
          <i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>


  <footer id="footer">
  <div class="footer-left">
    Copyright  &copy; 2025  You 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
        <li><a href="/">Home</a></li>
         
        <li><a href="/posts">Writings</a></li>
         
        <li><a href="/readings">Readings</a></li>
         
        <li><a href="/categories">Categories</a></li>
         
        <li><a href="/about">About</a></li>
        
      </ul>
    </nav>
  </div>
</footer>


  </div>
</body>

<link rel="stylesheet" href=/lib/font-awesome/css/all.min.css>
<script src=/lib/jquery/jquery.min.js></script>
<script src=/js/main.js></script>

<script src=/js/code-copy.js></script>




</html>
